{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur1565/.conda/envs/transformers4rec_v2_akis/lib/python3.10/site-packages/merlin/dtypes/mappings/tf.py:52: UserWarning: Tensorflow dtype mappings did not load successfully due to an error: No module named 'tensorflow'\n",
      "  warn(f\"Tensorflow dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/home/scur1565/.conda/envs/transformers4rec_v2_akis/lib/python3.10/site-packages/merlin/dtypes/mappings/triton.py:53: UserWarning: Triton dtype mappings did not load successfully due to an error: No module named 'tritonclient'\n",
      "  warn(f\"Triton dtype mappings did not load successfully due to an error: {exc.msg}\")\n",
      "/home/scur1565/.conda/envs/transformers4rec_v2_akis/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from models.t4rec_enriched import load_model, create_trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from dataloaders.extension import ParquetDataset, dataloader_collate_fn, preprocess\n",
    "from models.pci import PredictClickedInview\n",
    "\n",
    "from models.pci_attn import AttnPredictClickedInview\n",
    "\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "data_split = \"small\"\n",
    "model_trained_on = \"small\"\n",
    "\n",
    "per_device_train_batch_size = 2048\n",
    "per_device_eval_batch_size = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Projecting inputs of NextItemPredictionTask to'768' As weight tying requires the input dimension '64' to be equal to the item-id embedding dimension '768'\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATTENTION HEADS: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/scur1565/.conda/envs/transformers4rec_v2_akis/lib/python3.10/site-packages/merlin/io/dataset.py:267: UserWarning: Initializing an NVTabular Dataset in CPU mode.This is an experimental feature with extremely limited support!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 20\n",
    "output_dir = f\"../checkpoints/{data_split}-{model_trained_on}-extension\"\n",
    "\n",
    "if model_trained_on == \"small\":\n",
    "    model_path = \"../checkpoints/enriched/small/checkpoint-226/pytorch_model.bin\"\n",
    "else:\n",
    "    model_path = \"../checkpoints/enriched/large/checkpoint-23560/pytorch_model.bin\"\n",
    "\n",
    "model = load_model(model_path, max_sequence_length=max_sequence_length, d_model=64)\n",
    "model = model.cuda()\n",
    "params = {}\n",
    "params[\"max_inview\"] = 100\n",
    "params[\"user_feature_dim\"] = 0\n",
    "params[\"ovewrite_meta_model\"] = True\n",
    "params[\"use_summarizer\"] = True\n",
    "params[\"use_metadata\"] = False\n",
    "\n",
    "predict_model = AttnPredictClickedInview(model, params)\n",
    "\n",
    "predict_model = predict_model.cuda()\n",
    "\n",
    "trainer = create_trainer(\n",
    "    model,\n",
    "    output_dir,\n",
    "    epochs,\n",
    "    max_sequence_length,\n",
    "    per_device_train_batch_size,\n",
    "    per_device_eval_batch_size,\n",
    "    500,\n",
    ")\n",
    "\n",
    "train_paths = [\n",
    "        f\"../data/advanced_ts_ebnerd_processed/train/train_data_{data_split}_0.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/train/train_data_{data_split}_1.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/train/train_data_{data_split}_2.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/train/train_data_{data_split}_3.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/train/train_data_{data_split}_4.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/train/train_data_{data_split}_5.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/train/train_data_{data_split}_6.parquet\",\n",
    "    ]\n",
    "\n",
    "trainer.train_dataset_or_path = train_paths\n",
    "merlin_dataloader = trainer.get_train_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ParquetDataset(train_paths, [\"age\"], ['article_ids_inview', 'labels'], transform=preprocess)\n",
    "\n",
    "prq_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=per_device_train_batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    collate_fn=dataloader_collate_fn,\n",
    ")\n",
    "\n",
    "predict_model.train()\n",
    "loss_module = nn.BCEWithLogitsLoss(reduction='none')\n",
    "optimizer = torch.optim.SGD(\n",
    "    predict_model.parameters(), lr=1e-2, momentum=0.9\n",
    ")  # Default parameters, feel free to change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [02:14<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss:  tensor(0.3187, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [02:14<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss:  tensor(0.3010, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Finished training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting training\")\n",
    "for i in range(epochs):\n",
    "    predict_model.train()\n",
    "    epoch_loss = 0\n",
    "    for sample in zip(tqdm(merlin_dataloader), prq_dataloader):\n",
    "        # convert inputs to cuda\n",
    "        history, (user, inviews, length, labels) = sample\n",
    "        history = {name: value.cuda() for name, value in history[0].items()}\n",
    "        inviews, length = inviews.cuda(), length.cuda()\n",
    "        user = user.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = predict_model.forward(history, user, inviews, length)\n",
    "\n",
    "        loss = loss_module(preds.squeeze(-1), labels.float())\n",
    "\n",
    "        # Apply the mask to zero out the loss contributions from the padding elements\n",
    "        mask = (torch.arange(100).expand(len(length.cpu()), 100) < length.cpu().unsqueeze(1)).cuda()\n",
    "        masked_loss = loss * mask.float()\n",
    "\n",
    "        # Calculate the mean loss over the non-padding elements\n",
    "        mean_loss = masked_loss.sum() / mask.sum()\n",
    "        \n",
    "        epoch_loss += mean_loss\n",
    "        mean_loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(\"Epoch Loss: \", epoch_loss / len(merlin_dataloader))\n",
    "\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure that Validation Dataloader is deterministic: True\n"
     ]
    }
   ],
   "source": [
    "eval_paths = [\n",
    "        f\"../data/advanced_ts_ebnerd_processed/validation/validation_data_{data_split}_0.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/validation/validation_data_{data_split}_1.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/validation/validation_data_{data_split}_2.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/validation/validation_data_{data_split}_3.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/validation/validation_data_{data_split}_4.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/validation/validation_data_{data_split}_5.parquet\",\n",
    "        f\"../data/advanced_ts_ebnerd_processed/validation/validation_data_{data_split}_6.parquet\",\n",
    "    ]\n",
    "\n",
    "validation_df = pd.read_parquet(eval_paths, engine=\"pyarrow\")\n",
    "\n",
    "trainer.eval_dataset_or_path = eval_paths\n",
    "dlv = trainer.get_eval_dataloader()\n",
    "val_dataset = ParquetDataset(eval_paths, [\"age\"], ['article_ids_inview', 'labels'], transform=preprocess)\n",
    "\n",
    "val_dataset.max_inview_lenght = train_dataset.max_inview_lenght # set train max length\n",
    "\n",
    "eval_prq_dataloader = DataLoader(val_dataset, batch_size=per_device_eval_batch_size, shuffle=False, drop_last=False, collate_fn=dataloader_collate_fn)\n",
    "\n",
    "\n",
    "print(f\"Ensure that Validation Dataloader is deterministic: {not dlv.shuffle}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [01:28<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "all_inview_scores = []\n",
    "index: int = 0\n",
    "predict_model.eval()\n",
    "# Sample one from data loader\n",
    "for sp_batch in zip(tqdm(dlv), eval_prq_dataloader):\n",
    "    history, (user, inviews, length, labels) = sp_batch\n",
    "    history = {name: value.cuda() for name, value in history[0].items()}\n",
    "    inviews, length = inviews.cuda(), length.cuda()\n",
    "    user = user.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = predict_model.forward(history, user, inviews, length)\n",
    "    \n",
    "    preds = preds.squeeze(-1).cpu().detach().numpy()\n",
    "    preds = [p[:ln] for p, ln in zip(preds, length)]\n",
    "\n",
    "    all_inview_scores.extend(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.551068365070001,\n",
       "    \"mrr\": 0.34628988634116115,\n",
       "    \"ndcg@5\": 0.3833610563368501,\n",
       "    \"ndcg@10\": 0.46139431279504994\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "metrics = MetricEvaluator(\n",
    "    labels=validation_df[\"labels\"].to_list()[:-7],\n",
    "    predictions=all_inview_scores,\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers4rec_v2_akis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
